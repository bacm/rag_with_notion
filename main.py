from langchain.embeddings import HuggingFaceEmbeddings
from query import ask_question
from fetch_notion import get_chunks_and_model
from psql_helpers import populate
from langchain_community.chat_models import ChatOllama
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

PAGE_ID = "079277cc06e0484890da360181967cca"

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
chunks = get_chunks_and_model(PAGE_ID)
vectorstore = populate(chunks, embeddings)

# 1. Mod√®le LLM (Mixtral via Ollama)
llm = ChatOllama(model="mixtral")

# 2. M√©moire de conversation
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"
)

# 3. Retriever √† partir du vectorstore
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# 4. Cha√Æne RAG conversationnelle
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    return_source_documents=True,
    output_key="answer"
)


print("üß† Chatbot RAG pr√™t. Pose ta question (ou tape quit() pour quitter)")

while True:
    try:
        query = input("\n‚ùì Question : ")
        if query.strip().lower() == "quit()":
            print("üëã √Ä bient√¥t.")
            break
        res = qa_chain.invoke({"question": query})
        print(f"üí¨ R√©ponse : {res["answer"]}")
        print(res)
    except KeyboardInterrupt:
        print("\nüëã Interruption. √Ä bient√¥t.")
        break